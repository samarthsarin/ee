{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/moremoreurgent/Final_Training_More_Urgent - Copy.csv\n",
      "/kaggle/input/nlpword2vecembeddingspretrained/glove.6B.100d.txt\n",
      "/kaggle/input/nlpword2vecembeddingspretrained/glove.6B.200d.txt\n",
      "/kaggle/input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin\n",
      "/kaggle/input/nlpword2vecembeddingspretrained/glove.6B.300d.txt\n",
      "/kaggle/input/nlpword2vecembeddingspretrained/glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordninja\r\n",
      "  Downloading wordninja-2.0.0.tar.gz (541 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 541 kB 7.4 MB/s \r\n",
      "\u001b[?25hBuilding wheels for collected packages: wordninja\r\n",
      "  Building wheel for wordninja (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541552 sha256=b0022551a2daf95d92703f2ce0da12ca4026923c39ce91ffe9e6f62864e06c7d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/ff/82/e3dfc11e488be5de981117790251d3dd0c51544c8237b14499\r\n",
      "Successfully built wordninja\r\n",
      "Installing collected packages: wordninja\r\n",
      "Successfully installed wordninja-2.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Convolution1D,GlobalMaxPool1D,Attention,Dropout,Dense,Embedding,Bidirectional,GRU,GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import wordninja\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Bidirectional,GRU,Convolution1D,Dense,GlobalMaxPooling1D,Dropout\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Urgent</th>\n",
       "      <th>Clean Text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>reason- I disputed inaccurate unverified items...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>reason- disputed inaccurate unverified item re...</td>\n",
       "      <td>Urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>In 2016 I received notice of balance owing to ...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2016 received notice balance owing capital one...</td>\n",
       "      <td>Urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Hi my is name XXXX XXXX. I have some concerns ...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>hi name concern servicer select portfolio serv...</td>\n",
       "      <td>NotUrgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Specialized Loan Servicing ( SLS ) acquired my...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>specialized loan servicing sl acquired mortgag...</td>\n",
       "      <td>Urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>XXXX XXXX Chief Executive Officer and Presiden...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>chief executive officer president select portf...</td>\n",
       "      <td>NotUrgent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product  \\\n",
       "0  Credit reporting, credit repair services, or o...   \n",
       "1                                    Debt collection   \n",
       "2                                           Mortgage   \n",
       "3                                           Mortgage   \n",
       "4                                           Mortgage   \n",
       "\n",
       "                        Consumer complaint narrative  Sentiment Score  Urgent  \\\n",
       "0  reason- I disputed inaccurate unverified items...               22       1   \n",
       "1  In 2016 I received notice of balance owing to ...               14       1   \n",
       "2  Hi my is name XXXX XXXX. I have some concerns ...               13       1   \n",
       "3  Specialized Loan Servicing ( SLS ) acquired my...               13       1   \n",
       "4  XXXX XXXX Chief Executive Officer and Presiden...               12       1   \n",
       "\n",
       "                                          Clean Text     Target  \n",
       "0  reason- disputed inaccurate unverified item re...     Urgent  \n",
       "1  2016 received notice balance owing capital one...     Urgent  \n",
       "2  hi name concern servicer select portfolio serv...  NotUrgent  \n",
       "3  specialized loan servicing sl acquired mortgag...     Urgent  \n",
       "4  chief executive officer president select portf...  NotUrgent  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/moremoreurgent/Final_Training_More_Urgent - Copy.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove3ConsecutiveDuplicates(string): \n",
    "    val = \"\" \n",
    "    i = 0\n",
    "    while (i < len(string)): \n",
    "        if (i < len(string) - 2 and\n",
    "            string[i] * 3 == string[i:i + 3]): \n",
    "            i += 3\n",
    "        else: \n",
    "            val += string[i] \n",
    "            i += 1\n",
    "              \n",
    "    if (len(val) == len(string)): \n",
    "        return val \n",
    "    else: \n",
    "        return remove3ConsecutiveDuplicates(val)\n",
    "    \n",
    "#stop_words = list(set(stopwords.words('english')))+list(punctuation)+['unknown','xx']\n",
    "stop_words = ['unknown','xx']\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def removal(text):\n",
    "    text = re.sub('XXXX',' UNKNOWN ',text)\n",
    "    text = re.sub('XX/XX/','',text)\n",
    "    text = re.sub('UNKNOWN   UNKNOWN','UNKNOWN',text)\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = re.sub('  ',' ',text)\n",
    "    return text\n",
    "\n",
    "def cleaning(text):\n",
    "    text = removal(text)\n",
    "    #text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    #words = [w for w in words if len(w)>2]\n",
    "    words = [lem.lemmatize(w,'v') for w in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def total_clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^A-Za-z0-9]',' ',text)\n",
    "    text = cleaning(text)\n",
    "    text = wordninja.split(text)\n",
    "    text = ' '.join(text)\n",
    "    text = remove3ConsecutiveDuplicates(text)\n",
    "    #blob = TextBlob(text)\n",
    "    #text = blob.correct()\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    #text = word_tokenize(str(text))\n",
    "    #text = ' '.join([w for w in text if len(w)>1])\n",
    "    #text = re.sub('sap','asap',str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5305/5305 [00:56<00:00, 94.26it/s] \n"
     ]
    }
   ],
   "source": [
    "df['Total Clean'] = df['Clean Text'].progress_apply(total_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Urgent</th>\n",
       "      <th>Clean Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Total Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>reason- I disputed inaccurate unverified items...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>reason- disputed inaccurate unverified item re...</td>\n",
       "      <td>1</td>\n",
       "      <td>reason dispute inaccurate unverified item repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>In 2016 I received notice of balance owing to ...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2016 received notice balance owing capital one...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016 receive notice balance owe capital one ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Hi my is name XXXX XXXX. I have some concerns ...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>hi name concern servicer select portfolio serv...</td>\n",
       "      <td>0</td>\n",
       "      <td>hi name concern service  select portfolio serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Specialized Loan Servicing ( SLS ) acquired my...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>specialized loan servicing sl acquired mortgag...</td>\n",
       "      <td>1</td>\n",
       "      <td>specialize loan service sl acquire mortgage in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>XXXX XXXX Chief Executive Officer and Presiden...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>chief executive officer president select portf...</td>\n",
       "      <td>0</td>\n",
       "      <td>chief executive officer president select portf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product  \\\n",
       "0  Credit reporting, credit repair services, or o...   \n",
       "1                                    Debt collection   \n",
       "2                                           Mortgage   \n",
       "3                                           Mortgage   \n",
       "4                                           Mortgage   \n",
       "\n",
       "                        Consumer complaint narrative  Sentiment Score  Urgent  \\\n",
       "0  reason- I disputed inaccurate unverified items...               22       1   \n",
       "1  In 2016 I received notice of balance owing to ...               14       1   \n",
       "2  Hi my is name XXXX XXXX. I have some concerns ...               13       1   \n",
       "3  Specialized Loan Servicing ( SLS ) acquired my...               13       1   \n",
       "4  XXXX XXXX Chief Executive Officer and Presiden...               12       1   \n",
       "\n",
       "                                          Clean Text  Target  \\\n",
       "0  reason- disputed inaccurate unverified item re...       1   \n",
       "1  2016 received notice balance owing capital one...       1   \n",
       "2  hi name concern servicer select portfolio serv...       0   \n",
       "3  specialized loan servicing sl acquired mortgag...       1   \n",
       "4  chief executive officer president select portf...       0   \n",
       "\n",
       "                                         Total Clean  \n",
       "0  reason dispute inaccurate unverified item repo...  \n",
       "1  2016 receive notice balance owe capital one ca...  \n",
       "2  hi name concern service  select portfolio serv...  \n",
       "3  specialize loan service sl acquire mortgage in...  \n",
       "4  chief executive officer president select portf...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['Target']  = le.fit_transform(df['Target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4326\n",
       "1     979\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "x = df['Total Clean']\n",
    "y = df['Target']\n",
    "\n",
    "tokenizer.fit_on_texts(x)\n",
    "seq = tokenizer.texts_to_sequences(x)\n",
    "pad_seq = pad_sequences(seq,maxlen = 500,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9193"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('/kaggle/input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin', \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4774 samples, validate on 531 samples\n",
      "Epoch 1/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.4914 - accuracy: 0.7976\n",
      "Epoch 00001: val_loss improved from inf to 0.40800, saving model to weights-improvement-01-0.84.h5\n",
      "4774/4774 [==============================] - 63s 13ms/sample - loss: 0.4911 - accuracy: 0.7979 - val_loss: 0.4080 - val_accuracy: 0.8437\n",
      "Epoch 2/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8173\n",
      "Epoch 00002: val_loss improved from 0.40800 to 0.34497, saving model to weights-improvement-02-0.87.h5\n",
      "4774/4774 [==============================] - 58s 12ms/sample - loss: 0.4084 - accuracy: 0.8171 - val_loss: 0.3450 - val_accuracy: 0.8663\n",
      "Epoch 3/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8645\n",
      "Epoch 00003: val_loss improved from 0.34497 to 0.32412, saving model to weights-improvement-03-0.86.h5\n",
      "4774/4774 [==============================] - 62s 13ms/sample - loss: 0.3127 - accuracy: 0.8645 - val_loss: 0.3241 - val_accuracy: 0.8588\n",
      "Epoch 4/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.9023\n",
      "Epoch 00004: val_loss improved from 0.32412 to 0.27591, saving model to weights-improvement-04-0.88.h5\n",
      "4774/4774 [==============================] - 60s 13ms/sample - loss: 0.2458 - accuracy: 0.9022 - val_loss: 0.2759 - val_accuracy: 0.8795\n",
      "Epoch 5/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9262\n",
      "Epoch 00005: val_loss improved from 0.27591 to 0.26367, saving model to weights-improvement-05-0.88.h5\n",
      "4774/4774 [==============================] - 62s 13ms/sample - loss: 0.1890 - accuracy: 0.9263 - val_loss: 0.2637 - val_accuracy: 0.8832\n",
      "Epoch 6/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9516\n",
      "Epoch 00006: val_loss improved from 0.26367 to 0.26156, saving model to weights-improvement-06-0.89.h5\n",
      "4774/4774 [==============================] - 58s 12ms/sample - loss: 0.1451 - accuracy: 0.9516 - val_loss: 0.2616 - val_accuracy: 0.8851\n",
      "Epoch 7/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9662\n",
      "Epoch 00007: val_loss did not improve from 0.26156\n",
      "4774/4774 [==============================] - 59s 12ms/sample - loss: 0.1044 - accuracy: 0.9663 - val_loss: 0.2617 - val_accuracy: 0.8908\n",
      "Epoch 8/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9830\n",
      "Epoch 00008: val_loss improved from 0.26156 to 0.26043, saving model to weights-improvement-08-0.89.h5\n",
      "4774/4774 [==============================] - 61s 13ms/sample - loss: 0.0697 - accuracy: 0.9830 - val_loss: 0.2604 - val_accuracy: 0.8927\n",
      "Epoch 9/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9943\n",
      "Epoch 00009: val_loss did not improve from 0.26043\n",
      "4774/4774 [==============================] - 61s 13ms/sample - loss: 0.0436 - accuracy: 0.9941 - val_loss: 0.2899 - val_accuracy: 0.8870\n",
      "Epoch 10/10\n",
      "4768/4774 [============================>.] - ETA: 0s - loss: 0.0366 - accuracy: 0.9939\n",
      "Epoch 00010: val_loss did not improve from 0.26043\n",
      "4774/4774 [==============================] - 66s 14ms/sample - loss: 0.0366 - accuracy: 0.9939 - val_loss: 0.2992 - val_accuracy: 0.8964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0ee05b01d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,300,input_length=500,weights = [embedding_matrix],trainable = False))\n",
    "model.add(Bidirectional(GRU(32,return_sequences=True)))\n",
    "#model.add(Convolution1D(32,2,activation='relu'))\n",
    "#model.add(Convolution1D(64,3,activation = 'relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(32,activation = 'relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(pad_seq,y,batch_size = 32,epochs = 10,validation_split = 0.10,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"I have an unresolved asap mortgage issue with banker. Please take action to resolve the same.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99062777]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tokenizer.texts_to_sequences([t])\n",
    "b_pad = pad_sequences(b,maxlen = 500,padding='post',truncating='pre')\n",
    "results = model.predict(b_pad)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer_word2vec_more_more_urgent.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
